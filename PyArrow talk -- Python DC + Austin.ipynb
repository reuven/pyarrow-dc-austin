{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85037639-fdd8-4221-a2e2-bfe8b30382d3",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "1. Pandas, NumPy, and PyArrow\n",
    "2. Arrow -- what it is\n",
    "3. PyArrow for file loading/saving\n",
    "4. PyArrow as a back-end data type\n",
    "5. Transitioning to PyArrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d86a7b-5279-4186-9221-24b455510f7d",
   "metadata": {},
   "source": [
    "# Pandas, NumPy, and PyArrow\n",
    "\n",
    "NumPy has been around for about 20 years. It gives us the speed and efficiency of C data types, but a Python API. That means we can use Python for numeric computing.\n",
    "\n",
    "NumPy defines the \"NumPy array,\" which contains a number of values all of the same type. These are all stored in C, which means that (in contrast with normal Python programming), we have to specify not only the type of values, but their lengths, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158ac9fa-7620-4c9a-8400-110d3d827fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fc076a-b2d3-4bbf-af6c-4ec6598ab460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 20, 30, 40, 50])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([10, 20, 30, 40, 50])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e8856b2-c772-4224-8a30-08feb9e65e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c30de43-cf18-4c62-947b-0830f6c51f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "514cb586-8746-4d90-8478-884a6b59b713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2079ff58-3d07-43b9-8fd8-a10b58172744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = [10, 20, 30, 40, 50]\n",
    "import sys\n",
    "sys.getsizeof(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ab7b3f-c6d5-49f0-9f50-3b6e9359212c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 20, 30, 40, 50], dtype=int8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([10, 20, 30, 40, 50], dtype='int8')\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17cf95-6b56-4a22-a67e-b56ed2e5ceaa",
   "metadata": {},
   "source": [
    "NumPy is great! But it's also a bit low level. About 15 years ago, Wes McKinney invented Pandas, which is a wrapper around NumPy, giving us a lot of convenience methods. NumPy handles n-dimensional arrays. Pandas has only two main data structures, one is the Series (1D) and also the DataFrame (2D).\n",
    "\n",
    "Pandas gives us lots of methods for:\n",
    "\n",
    "- Loading data from different formats\n",
    "- Storing data in different formats\n",
    "- Analyzing in many ways\n",
    "- Cleaning our data\n",
    "- Visualizing our data\n",
    "\n",
    "When we store data in our Pandas series or data frame, it's being stored in a NumPy array behind the scenes:\n",
    "\n",
    "- A series is actually a wrapper around a single NumPy array\n",
    "- Each of a data frame's columns is a NumPy array, but there are very fast transformations that allow us to retrieve by row, as well as by column.\n",
    "\n",
    "For many years, this has worked just fine... mostly. Where are the problems?\n",
    "\n",
    "- Strings -- currently, Pandas stores text as Python strings, with pointers from NumPy arrays into Python memory\n",
    "- There's no compression or general categorization/normalization of textual (or other) data\n",
    "- Much of the analysis we want to do is columnar, and NumPy just isn't that fast at working in columns, even when we think of a data frame as containing multiple series (i.e., NumPy arrays)\n",
    "- NumPy is great for Python, but it's not interoperable with other systems.\n",
    "\n",
    "Wes and others said: Let's create a new data structure that'll work in Pandas and also in lots of other systems and langauges. That became Apache Arrow, an open-source project. It just released version 20 a month or so ago.\n",
    "\n",
    "It supports many data structures (arrays, tables, dictionaries, etc.), with copy-on-write, compression, with reduction of duplications, and also nullable types. Python supports it through PyArrow, its bindings, but you also have Arrow compatibility with R, Apache Spark, and other languages/systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e6fcee-61ad-440f-aa2f-4a7836bf27b6",
   "metadata": {},
   "source": [
    "# PyArrow and Pandas\n",
    "\n",
    "For a long time, you could convert a PyArrow Table to a Pandas data frame, and vice versa. They were interoperable, but needed translation.\n",
    "\n",
    "A few years ago, several Pandas core developers decided it would be worth starting the journey of moving Pandas from NumPy to PyArrow as a back end.\n",
    "\n",
    "The idea is that Pandas will continue to work as before, with all of its methods and functionality, but the back end will be PyArrow, rather than NumPy.\n",
    "\n",
    "In a few years, we can expect that PyArrow will be the default back-end storage. I don't think that NumPy is going away, either in general or in Pandas. But Pandas 3.0 will require PyArrow to be installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897cb992-f7be-4b16-abe6-1092bf808f8f",
   "metadata": {},
   "source": [
    "# Loading/saving files\n",
    "\n",
    "Already today, in a non-experimental way, you can use PyArrow in your Pandas code to read from CSV files (and write to them, too).\n",
    "\n",
    "If you use `pd.read_csv`, then Pandas needs to do several things:\n",
    "\n",
    "- Load the file into memory, often in multiple chunks, if it's too big\n",
    "- Evaluate what dtype you're going to want for each column (if you don't specify), choosing between `int64`, `float64`, and `object` (i.e., string)\n",
    "- Create the data frame based on that\n",
    "- Pandas, of course, uses a single core, with a single process and a single thread\n",
    "\n",
    "PyArrow uses the Arrow engine, which uses multiple cores, and multiple threads, to read from a file, which makes it far faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f41918dc-c9be-4d56-a4e9-7b442b9882af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 reuven staff 656M Jul 29  2021 /Users/reuven/Courses/Current/Data/nyc_taxi_2019-01.csv\n"
     ]
    }
   ],
   "source": [
    "filename = '/Users/reuven/Courses/Current/Data/nyc_taxi_2019-01.csv'\n",
    "\n",
    "!ls -lh $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7ceab1d-51ae-4e27-9836-e7b602a25107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5cb87d3-7797-4287-9c3f-510ea81d1ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 6.42\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "df = pd.read_csv(filename)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "print(f'Total time: {total_time:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade0abf-ffc1-4831-a114-d59e61f9523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "df = pd.read_csv(filename)\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "print(f'Total time: {total_time:0.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
